
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_equivariant_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:


Self-supervised learning with Equivariant Imaging for MRI.
====================================================================================================

This example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.

The equivariant imaging loss is presented in `"Equivariant Imaging: Learning Beyond the Range Space"
<http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`_.

.. GENERATED FROM PYTHON SOURCE LINES 11-21

.. code-block:: Python


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.optim.prior import PnP
    from deepinv.utils.demo import load_dataset, load_degradation
    from deepinv.models.utils import get_weights_url








.. GENERATED FROM PYTHON SOURCE LINES 22-25

Setup paths for data loading and results.
---------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 25-36

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 37-46

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------
In this example, we use a subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_
as the base image dataset. It consists of 973 knee images of size 320x320.

.. note::

      We reduce to the size to 128x128 for faster training in the demo.


.. GENERATED FROM PYTHON SOURCE LINES 46-60

.. code-block:: Python


    operation = "MRI"
    train_dataset_name = "fastmri_knee_singlecoil"
    img_size = 128

    transform = transforms.Compose([transforms.Resize(img_size)])

    train_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True
    )
    test_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/fastmri_knee_singlecoil.pt
      0%|          | 0.00/399M [00:00<?, ?iB/s]      1%|          | 3.19M/399M [00:00<00:12, 31.8MiB/s]      3%|▎         | 10.7M/399M [00:00<00:06, 57.0MiB/s]      5%|▍         | 18.0M/399M [00:00<00:05, 64.7MiB/s]      6%|▋         | 25.4M/399M [00:00<00:05, 68.3MiB/s]      8%|▊         | 32.9M/399M [00:00<00:05, 70.6MiB/s]     10%|█         | 40.3M/399M [00:00<00:04, 71.8MiB/s]     12%|█▏        | 47.8M/399M [00:00<00:04, 72.8MiB/s]     14%|█▍        | 55.3M/399M [00:00<00:04, 73.5MiB/s]     16%|█▌        | 62.8M/399M [00:00<00:04, 74.1MiB/s]     18%|█▊        | 70.3M/399M [00:01<00:04, 74.3MiB/s]     20%|█▉        | 77.8M/399M [00:01<00:04, 74.4MiB/s]     21%|██▏       | 85.2M/399M [00:01<00:04, 74.2MiB/s]     23%|██▎       | 92.6M/399M [00:01<00:04, 71.9MiB/s]     25%|██▌       | 100M/399M [00:01<00:04, 72.4MiB/s]      27%|██▋       | 107M/399M [00:01<00:04, 72.7MiB/s]     29%|██▉       | 115M/399M [00:01<00:03, 73.3MiB/s]     31%|███       | 122M/399M [00:01<00:03, 73.9MiB/s]     33%|███▎      | 130M/399M [00:01<00:03, 74.0MiB/s]     34%|███▍      | 137M/399M [00:01<00:03, 74.3MiB/s]     36%|███▋      | 145M/399M [00:02<00:03, 74.5MiB/s]     38%|███▊      | 152M/399M [00:02<00:03, 74.7MiB/s]     40%|████      | 160M/399M [00:02<00:03, 74.5MiB/s]     42%|████▏     | 167M/399M [00:02<00:03, 74.6MiB/s]     44%|████▍     | 175M/399M [00:02<00:03, 74.6MiB/s]     46%|████▌     | 182M/399M [00:02<00:02, 74.4MiB/s]     48%|████▊     | 190M/399M [00:02<00:02, 74.2MiB/s]     49%|████▉     | 197M/399M [00:02<00:02, 73.6MiB/s]     51%|█████▏    | 205M/399M [00:02<00:02, 76.0MiB/s]     54%|█████▎    | 213M/399M [00:02<00:02, 77.8MiB/s]     56%|█████▌    | 221M/399M [00:03<00:02, 78.8MiB/s]     58%|█████▊    | 230M/399M [00:03<00:02, 79.7MiB/s]     60%|█████▉    | 238M/399M [00:03<00:01, 80.4MiB/s]     62%|██████▏   | 246M/399M [00:03<00:01, 80.6MiB/s]     64%|██████▍   | 254M/399M [00:03<00:01, 80.8MiB/s]     66%|██████▌   | 262M/399M [00:03<00:01, 80.7MiB/s]     68%|██████▊   | 270M/399M [00:03<00:01, 80.9MiB/s]     70%|██████▉   | 278M/399M [00:03<00:01, 81.1MiB/s]     72%|███████▏  | 287M/399M [00:03<00:01, 81.5MiB/s]     74%|███████▍  | 295M/399M [00:03<00:01, 81.8MiB/s]     76%|███████▌  | 303M/399M [00:04<00:01, 81.9MiB/s]     78%|███████▊  | 311M/399M [00:04<00:01, 81.9MiB/s]     80%|████████  | 320M/399M [00:04<00:00, 81.8MiB/s]     82%|████████▏ | 328M/399M [00:04<00:00, 81.8MiB/s]     84%|████████▍ | 336M/399M [00:04<00:00, 82.0MiB/s]     86%|████████▋ | 344M/399M [00:04<00:00, 81.9MiB/s]     88%|████████▊ | 352M/399M [00:04<00:00, 81.9MiB/s]     90%|█████████ | 361M/399M [00:04<00:00, 81.8MiB/s]     93%|█████████▎| 369M/399M [00:04<00:00, 81.3MiB/s]     95%|█████████▍| 377M/399M [00:04<00:00, 81.1MiB/s]     97%|█████████▋| 385M/399M [00:05<00:00, 81.1MiB/s]     99%|█████████▊| 393M/399M [00:05<00:00, 81.2MiB/s]    100%|██████████| 399M/399M [00:05<00:00, 76.8MiB/s]




.. GENERATED FROM PYTHON SOURCE LINES 61-65

Generate a dataset of knee images and load it.
----------------------------------------------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 65-95

.. code-block:: Python


    mask = load_degradation("mri_mask_128x128.npy", ORIGINAL_DATA_DIR)

    # defined physics
    physics = dinv.physics.MRI(mask=mask, device=device)

    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        900 if torch.cuda.is_available() else 5
    )  # number of images used for training
    # (the dataset has up to 973 images, however here we use only 900)

    my_dataset_name = "demo_equivariant_imaging"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    mri_mask_128x128.npy degradation downloaded in datasets
    Dataset has been saved in measurements/fastmri_knee_singlecoil/MRI




.. GENERATED FROM PYTHON SOURCE LINES 96-101

Set up the reconstruction network
---------------------------------------------------------------

As a reconstruction network, we use an unrolled network (half-quadratic splitting)
with a trainable denoising prior based on the DnCNN architecture.

.. GENERATED FROM PYTHON SOURCE LINES 101-146

.. code-block:: Python


    # Select the data fidelity term
    data_fidelity = dinv.optim.L2()
    n_channels = 2  # real + imaginary parts

    # If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each
    # iteration. For fixed trained model prior across iterations, initialize with a single model.
    prior = PnP(
        denoiser=dinv.models.DnCNN(
            in_channels=n_channels,
            out_channels=n_channels,
            pretrained=None,
            train=True,
            depth=7,
        ).to(device)
    )

    # Unrolled optimization algorithm parameters
    max_iter = 3  # number of unfolded layers
    lamb = [1.0] * max_iter  # initialization of the regularization parameter
    stepsize = [1.0] * max_iter  # initialization of the step sizes.
    sigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
    }

    trainable_params = [
        "lambda",
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = dinv.unfolded.unfolded_builder(
        "HQS",
        params_algo=params_algo,
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )









.. GENERATED FROM PYTHON SOURCE LINES 147-160

Set up the training parameters
--------------------------------------------
We choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)
and the equivariant imaging loss (EI).
The EI loss requires a group of transformations to be defined. The forward model `should not be equivariant to
these transformations <https://www.jmlr.org/papers/v24/22-0315.html>`_.
Here we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is
not equivariant to rotations (while it is equivariant to translations).

.. note::

      We use a pretrained model to reduce training time. You can get the same results by training from scratch
      for 150 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 160-185

.. code-block:: Python


    epochs = 1  # choose training epochs
    learning_rate = 5e-4
    batch_size = 16 if torch.cuda.is_available() else 1

    # choose self-supervised training losses
    # generates 4 random rotations per image in the batch
    losses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)

    # start with a pretrained model to reduce training time
    file_name = "new_demo_ei_ckp_150_v3.pth"
    url = get_weights_url(model_name="demo", file_name=file_name)
    ckpt = torch.hub.load_state_dict_from_url(
        url,
        map_location=lambda storage, loc: storage,
        file_name=file_name,
    )
    # load a checkpoint to reduce training time
    model.load_state_dict(ckpt["state_dict"])
    optimizer.load_state_dict(ckpt["optimizer"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/demo/resolve/main/new_demo_ei_ckp_150_v3.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/new_demo_ei_ckp_150_v3.pth
      0%|          | 0.00/2.17M [00:00<?, ?B/s]    100%|██████████| 2.17M/2.17M [00:00<00:00, 32.2MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 186-189

Train the network
--------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 189-221

.. code-block:: Python



    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False
    )

    # Initialize the trainer
    trainer = dinv.Trainer(
        model,
        physics=physics,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        optimizer=optimizer,
        train_dataloader=train_dataloader,
        plot_images=True,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.
        ckp_interval=10,
    )

    model = trainer.train()




.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :alt: Backprojection, Output, Target
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
    Train epoch 0: MCLoss=0.0, EILoss=0.0, TotalLoss=0.0, PSNR=36.182




.. GENERATED FROM PYTHON SOURCE LINES 222-226

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 226-228

.. code-block:: Python


    trainer.test(test_dataloader)



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_002.png
   :alt: Input, No learning, Recons., GT
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Test PSNR: No learning rec.: 29.389+-3.411 | Model: 35.264+-2.623. 

    (35.263515028235034, 2.6233009603086459, 29.388799536718082, 3.4114185158882959)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 19.814 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_equivariant_imaging.ipynb <demo_equivariant_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_equivariant_imaging.py <demo_equivariant_imaging.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
